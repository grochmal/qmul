\documentclass[11pt,a4paper,twoside,openright]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}

\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{booktabs}
%\usepackage[table]{xcolor}

\title{Multi'omics analysis: three month report}
\author{Michal Grochmal
  $<$\href{mailto:grochmal@member.fsf.org}{grochmal@member.fsf.org}$>$\\
  \\
  Department of Electronic Engineering and Computer Science\\
  Queen Mary University of London
}
\date{December 2014}

\usepackage[ colorlinks=true
           , plainpages=false
           , pdfpagelabels
           , pageanchor=false ]{hyperref}

\setlength{\headheight}{16pt}

\begin{document}
\maketitle

\newpage
\null
\thispagestyle{empty}
\newpage

\pagestyle{fancy}
\lhead{}
\chead{MULTI'OMICS ANALYSIS: THREE MONTH REPORT}
\rhead{}

\begin{abstract}
Experimental data of a single set of runs of one experiment can prove tricky to
interpret, yet in most single experiments we expect a result confirming or
denying a predefined hypothesis.  Building interpretations on data from several
experiments at once faces extra issues: high dimensionality if we concatenate
the data, missing data in different measures across the different experiments
and correlation of the measures provided by each experiment.  Classical and
recent methods to integrate data from several sources may prove effective to
conquer these difficulties.  We evaluate known methods of scaling, clustering
and regression in the task of integrating such sources.
\begin{flushright}
\emph{Similia similibus curantur -- Paracelsus}
\end{flushright}
\end{abstract}

\newpage
\null
\thispagestyle{empty}
\newpage

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

% clever trick to clear the pages before chapter openings
\clearpage{\pagestyle{empty}\cleardoublepage}
\chapter{Introduction}

Why is data integration useful?  Texts that use data integration rarely provide
real world examples, which are often intuitive to understand, this leaves the
usefulness of data integration hidden in complex terms.  Lets think of a
situation where plain classification would perform badly, but an classification
on integrated data will have better results:  Imagine we are trying to classify
several \emph{motor vehicles} into "sport vehicles" and "no-sport vehicles",
our entire set of vehicles is composed of models of cars and motorbikes
containing the data from their manufacturer website; the data is labeled with
the class names "sport" and "non-sport".  The complication is that measures
that distinguish sport cars from non-sport cars are often different from the
measures that distinguish sport bikes from non-sport bikes, e.g. sport cars are
heavier than most cars whilst sport bikes are lighter than most bikes.

Vertical concatenation (a union in relational algebra) of measures for cars and
bikes will result if a very low classification performance.  Horizontal
concatenation (a projection in relational algebra) of the measures might result
in better classification but still lower than if we integrate the data.  Within
the example about cars and motorbikes we can finally define \emph{what
integration of data means}:  We try to understand the correlation (or another
measure of similarity) between the measures in two (or more) related datasets
(car features and motorbike features), from this understanding we give high
weights to the measures that are positively correlated and give low weights to
the measures that are negatively correlated between the sets.  The weight of
the vehicle is an example of a measure that is negatively correlated between
cars and motorbikes in the context of "sport vehicle" and "not sport vehicle"
classification;  This is because heavier cars tend to be sport cars and lighter
motorbikes tend to be sport bikes.  On the other hand, the measure of wheel
diameter is positively correlated as both sport cars and sport bikes tend to
have bigger wheels than non-sport ones.

The cars and motorbikes classification above is one case where data integration
is useful.  Another example, still using vehicles would be to \emph{integrate
data from various sources about the same vehicle}, taking into consideration
only cars for example.  This second example is a different scenario because we
are not integrating two datasets of different points in space, we are
integrating two datasets that use the same set of points in space but different
sets of measures.  In the first example one dataset contains data about cars
such as "Subaru 20" or "Mazda 1.5" and the other dataset contains data about
motorbikes such as "Java 350cc" or "Kawasaki Marauder".  The second scenario
integrates a dataset that has measures for "Subaru 20" and "Mazda 1.5" with
another dataset that contains different measures for the same "Subaru 20" and
"Mazda 1.5" models.

Imagine we will integrate data from several sources about a single car (we
forget about motorbikes for a moment) and then classify it as \emph{sport} or
\emph{non-sport} vehicle.  The data integration in this second example is not
happening between different types of vehicles but between different sources of
information for a single vehicle.  Lets try to build a classifier for
\emph{sport} and \emph{non-sport} cars based on vehicle data from the
manufacturer website, from an evaluation website (a website that makes rankings
of cars) and from a forum of car enthusiasts.

The data from the manufacturer website, the evaluation website and the forum is
collected for each car model.  \emph{Vertical concatenation} does not make
sense in this case, because each model would be present in the result of the
concatenation three times.  Also, the measures cannot be inserted into the same
columns when coming from different sources.  \emph{Horizontal concatenation}
will produce a set where the number of measures might be high.  e.g. We could
collect the data for a total of 16 car models from the three websites; from the
vendor we collect 7 measures, from the evaluation we collect 10 measures and
from the forum we collect 5 measures for each model.  After horizontal
concatenation the resulting dataset has 16 data points (car models) and 32
measures.  Classification in a set where the number of dimensions (measures) is
bigger than the number of data points is often bad, this is called the problem
of high dimensionality.

One way to overcome the high dimensionality problem is to build three
classifiers:  one classifier for the measures from the manufacturer website,
one classifier for the evaluation website and one classifier for the forum
data.  The final classifier will then use the result (\emph{sport} or
\emph{non-sport}) of each of the three classifiers and decide based on the
majority vote.  This is an \emph{ensemble} approach for integration.

In data integration terms the approaches used in the vertical and horizontal
concatenations in the classification of both cars and motorbikes, and in the
horizontal concatenation of the car data from multiple sources are all called
\emph{early integration}.  The application of weights to different measures of
cars and motorbikes (based on the correlation) in the first example is an
\emph{intermediate integration} technique.  And finally the ensemble approach
is called a \emph{late integration}.

% clever trick to clear the pages before chapter openings
\clearpage{\pagestyle{empty}\cleardoublepage}
\chapter{Integration}

Data integration is not a technique by itself, it is a collection of more or
less related techniques that perform \emph{operations on data from several
sources} with the objective of \emph{bestowing classification accuracy} on
these sources.  The integration is needed as simple concatenation of data often
become problematic.  Techniques as concatenation followed by dimensionality
reduction are a simple form of data integration.

We can divide data integration into early integration, which combines the data
\emph{before} performing any operation on it (not counting the normalisation of
the data).  Late integration, which \emph{combines classification results} from
classifiers on each of the data sources instead of combining the data itself.
And intermediate integration which first performs several operations on the
data, combining it \emph{into a single data set}, and then performs
classification on the entirety of the data.

\section{Purpose}

Several areas benefit from methods of data integration.  Any area where we need
to combine data from diverse sources (either as separate observations or as
different measures about the same observations) can benefit from
differentiating the integration of data performed by early, intermediate or
late integration.  Experimental data from areas of science is one such area
that benefit from different ways of putting the data together.  One of the
biggest experimental datasets that is openly available are \emph{biological
databases} of multi'omics.  High dimensionality (as we have seen in the
introduction) is a common problem in the multi'omics.  Also, multi'omics
present several other problems: missing data for certain observations and lack
of real world, intuitive, models for the relations between data.

Another, not data integration related, problem of the multi'omics data is a
lack of standard formats for the data.  Standards do exist, but there are
hundreds of them and not all databases of multi'omics data follow them
strictly.  Even more, some of the standard formats are human readable, but
\emph{not machine readable}.  This is a huge computational problem of the
processing of the data.

For example, imagine that the central London laboratory will perform blood
tests for all London citizens registered in London GPs (General Practice
medical posts).  The GPs are asked to collect blood from all patients
registered with them but are \emph{not told the procedure} to follow when
collecting the blood.  Some GPs collect the blood of the patients in the
morning, some others after lunch, even other GPs tell the patients to not eat
for 12 hours before blood collection.  Each GP sends the blood samples and a
letter describing how the blood was collected.  The central London laboratory
cannot perform all blood test after receiving the blood, because different
tests have different requirements of the time a patient remained without
eating.  Yet, \emph{experts} at the laboratory can guess from the letter sent
by the GP which blood tests can be performed for each patient.  If all possible
tests were performed on all samples the results would be meaningless.

A computer would not be capable to decide which tests can be performed as it
does not have the expert's knowledge and it cannot understand the \emph{natural
language} letters are written in, especially in an expert context.  This GP
analogy reflects the multi'omics databases organisation, these databases are
made for the human reader.  A human expert needs to evaluate the provenience
and location of the data and, only then, decide what can be done with that
data.  These databases are standardised for a human reader not for a machine
reader.

The work of standardising the multi'omics databases is out of the scope of this
work, although it is a huge problem in the multi'omics area.  We focus on the
methods for data integration, therefore we assume that we \emph{posses}, and
understand, the data on which we work.

\section{Methods}

Methods for early integration are comprised of concatenating and normalising
the data.  Late integration is most often performed by ensemble methods, these
use different ways of evaluating the results of each classification, e.g.
majority vote or cumulative vote.  Yet techniques for \emph{intermediate
integration} are more diverse.  Table \ref{tab:tech} summarises some methods
for intermediate integration.

\begin{table}[hp]
\centering
%\rowcolors{3}{}{lightgray}
\begin{tabular}{|l|l|l|}
\toprule
Technique & \multirow{2}{0.3\textwidth}{Technique}
          & \multirow{2}{0.39\textwidth}{Characteristics} \\
Type & & \\
\midrule

\multirow{6}{0.2\textwidth}{Clustering}
& \multirow{6}{0.3\textwidth}{Parallel clustering\cite{greene2008}}
  & Use k-means to generate \\
& & clusters from each \\
& & data source. \\
\cline{3-3}
& & Mix the clusters, \\
& & using convolutely \\
& & explained techniques. \\
\hline

\multirow{6}{0.2\textwidth}{Euclidean distance}
& \multirow{6}{0.3\textwidth}{Multidimensional scaling\cite{kruskal1978}}
  & Uses euclidean distance \\
& & between the data points. \\
\cline{3-3}
& & Minimises the error \\
& & of achieving the same \\
& & distances in a smaller \\
& & number of dimensions. \\
\hline

\multirow{10}{0.2\textwidth}{Graph based}
& \multirow{7}{0.3\textwidth}{Similarity network fusion\cite{wang2014}}
  & Weighted edges \\
& & representing pairwise \\
& & distances between \\
& & data points. \\
\cline{3-3}
& & Mix edge weights \\
& & between the sources \\
& & being integrated. \\
\cline{2-3}
& \multirow{3}{0.3\textwidth}{Spectral clustering\cite{luxburg2007}}
  & Uses laplacian matrix\\
& & for representation \\
& & of the graph. \\
\hline

\multirow{2}{0.2\textwidth}{Bayesian}
& \multirow{2}{0.3\textwidth}{Bayesian correlation\cite{kirk2012}}
  & Uses a Bayesian \\
& & network \\
\hline

\multirow{9}{0.2\textwidth}{Vectorial space}
& \multirow{9}{0.3\textwidth}{Partial least squares\cite{siongng2013}}
  & Maximum variance \\
& & is calculated \\
& & in vectorial space. \\
\cline{3-3}
& & The maximum variance \\
& & is removed and \\
& & a new maximum is \\
& & recalculated, until \\
& & all dimensions are \\
& & accounted for. \\

\bottomrule
\end{tabular}
\caption{Classification of techniques used in data integration.}
\label{tab:tech}
\end{table}

% clever trick to clear the pages before chapter openings
\clearpage{\pagestyle{empty}\cleardoublepage}
\bibliographystyle{plain}
\bibliography{threem}

\appendix

% clever trick to clear the pages before chapter openings
\clearpage{\pagestyle{empty}\cleardoublepage}
\chapter{Plan A}

1st half of January: replicate SNF on GBM data

2nd half of January: apply MDS to the same data

1st half of February: graph both techniques

% clever trick to clear the pages before chapter openings
\clearpage{\pagestyle{empty}\cleardoublepage}
\chapter{Plan B}

1st half of January: replicate SNF on GBM data

2nd half of January: make a model of distributed SNF

1st half of February: check if the distributed model is properly distributed

2nd half of February: check if the distributed model is resistant to failures

% clever trick to clear the pages before chapter openings
\clearpage{\pagestyle{empty}\cleardoublepage}
\chapter{Plan C}

1st half of January: understand assembler construction of ELF objects

2nd half of January: build a couple of test programs hand linked into ELFs

1st half of February: build a prototype kernel that can start from a bootloader
and then HALT the machine.

\end{document}

